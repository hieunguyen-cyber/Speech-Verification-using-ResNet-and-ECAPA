{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9920101,"sourceType":"datasetVersion","datasetId":6096635},{"sourceId":11565412,"sourceType":"datasetVersion","datasetId":7251391},{"sourceId":376270,"sourceType":"modelInstanceVersion","modelInstanceId":309912,"modelId":330281},{"sourceId":376443,"sourceType":"modelInstanceVersion","modelInstanceId":309912,"modelId":330281}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## ECAPA Architecture","metadata":{}},{"cell_type":"markdown","source":"### SE-Module","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass SEModule(nn.Module):\n    def __init__(self, channels, bottleneck=128):\n        \"\"\"\n        Squeeze-Excitation Module for channel-wise attention\n        \n        Args:\n            channels: Number of input channels\n            bottleneck: Dimension of the bottleneck representation\n        \"\"\"\n        super(SEModule, self).__init__()\n        self.se = nn.Sequential(\n            nn.AdaptiveAvgPool1d(1),\n            nn.Conv1d(channels, bottleneck, kernel_size=1, padding=0),\n            nn.ReLU(),\n            nn.Conv1d(bottleneck, channels, kernel_size=1, padding=0),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        se = self.se(x)\n        return x * se","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T11:26:17.946063Z","iopub.execute_input":"2025-05-06T11:26:17.946581Z","iopub.status.idle":"2025-05-06T11:26:17.951827Z","shell.execute_reply.started":"2025-05-06T11:26:17.946556Z","shell.execute_reply":"2025-05-06T11:26:17.951229Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Res2Block","metadata":{}},{"cell_type":"code","source":"class Res2Block(nn.Module):\n    def __init__(self, channels, kernel_size=3, dilation=1, scale=8):\n        \"\"\"\n        Res2Net Block for multi-scale feature extraction\n        \n        Args:\n            channels: Number of input/output channels\n            kernel_size: Size of the convolutional kernel\n            dilation: Dilation rate for the convolutions\n            scale: Number of scales for feature extraction\n        \"\"\"\n        super(Res2Block, self).__init__()\n        self.scale = scale\n        self.width = channels // scale\n        self.nums = scale if channels % scale == 0 else scale - 1\n\n        self.convs = nn.ModuleList()\n        for i in range(self.nums):\n            self.convs.append(\n                nn.Conv1d(\n                    self.width,\n                    self.width,\n                    kernel_size=kernel_size,\n                    dilation=dilation,\n                    padding=dilation * (kernel_size - 1) // 2,\n                )\n            )\n\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        out = []\n        spx = torch.split(x, self.width, 1)\n        for i in range(self.nums):\n            if i == 0:\n                sp = spx[i]\n            else:\n                sp = sp + spx[i]\n            sp = self.relu(self.convs[i](sp))\n            out.append(sp)\n        if self.scale - self.nums > 0:\n            out.append(spx[self.nums])\n        out = torch.cat(out, dim=1)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T11:26:24.445431Z","iopub.execute_input":"2025-05-06T11:26:24.445694Z","iopub.status.idle":"2025-05-06T11:26:24.452632Z","shell.execute_reply.started":"2025-05-06T11:26:24.445673Z","shell.execute_reply":"2025-05-06T11:26:24.452048Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### SE-Res2Block","metadata":{}},{"cell_type":"code","source":"class SERes2Block(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, scale=8):\n        \"\"\"\n        SE-Res2Block: Combines Res2Net with Squeeze-Excitation\n        \n        Args:\n            in_channels: Number of input channels\n            out_channels: Number of output channels\n            kernel_size: Size of the convolutional kernel\n            dilation: Dilation rate for the convolutions\n            scale: Number of scales for Res2Net\n        \"\"\"\n        super(SERes2Block, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n        self.res2block = Res2Block(out_channels, kernel_size, dilation, scale)\n        self.se = SEModule(out_channels)\n        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        residual = x\n        x = self.conv1(x)\n        x = self.res2block(x)\n        x = self.se(x)\n        x = self.conv2(x)\n        return x + residual","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T11:26:26.264576Z","iopub.execute_input":"2025-05-06T11:26:26.265212Z","iopub.status.idle":"2025-05-06T11:26:26.270098Z","shell.execute_reply.started":"2025-05-06T11:26:26.265189Z","shell.execute_reply":"2025-05-06T11:26:26.269302Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Attentive Stats Pool","metadata":{}},{"cell_type":"code","source":"class AttentiveStatsPooling(nn.Module):\n    def __init__(self, in_dim, bottleneck_dim=128):\n        \"\"\"\n        Attentive Statistics Pooling\n        \n        Args:\n            in_dim: Number of input channels\n            bottleneck_dim: Dimension of the bottleneck in the attention mechanism\n        \"\"\"\n        super(AttentiveStatsPooling, self).__init__()\n        self.attention = nn.Sequential(\n            nn.Conv1d(in_dim, bottleneck_dim, kernel_size=1),\n            nn.ReLU(),\n            nn.Conv1d(bottleneck_dim, in_dim, kernel_size=1),\n            nn.Softmax(dim=2),\n        )\n\n    def forward(self, x):\n        # x is (batch, channels, time)\n        attention_weights = self.attention(x)\n        \n        # Weighted mean\n        mean = torch.sum(x * attention_weights, dim=2)\n        \n        # Weighted standard deviation\n        var = torch.sum(x**2 * attention_weights, dim=2) - mean**2\n        std = torch.sqrt(var.clamp(min=1e-5))\n        \n        # Concatenate mean and standard deviation\n        pooled = torch.cat([mean, std], dim=1)\n        return pooled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T11:26:27.804988Z","iopub.execute_input":"2025-05-06T11:26:27.80545Z","iopub.status.idle":"2025-05-06T11:26:27.810748Z","shell.execute_reply.started":"2025-05-06T11:26:27.805426Z","shell.execute_reply":"2025-05-06T11:26:27.810196Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ECAPA_TDNN","metadata":{}},{"cell_type":"code","source":"class ECAPA_TDNN(nn.Module):\n    def __init__(\n        self,\n        input_dim=80,         # Input feature dimension (e.g., 80 filterbank features)\n        channels=512,         # Number of channels in the convolutional blocks\n        embedding_dim=192,    # Final embedding dimension\n        res2net_scale=8,      # Scale parameter for Res2Net blocks\n    ):\n        \"\"\"\n        ECAPA-TDNN model for speaker verification\n        \n        Args:\n            input_dim: Dimension of the input features\n            channels: Number of channels in the convolutional blocks\n            embedding_dim: Dimension of the final speaker embedding\n            res2net_scale: Scale parameter for Res2Net blocks\n        \"\"\"\n        super(ECAPA_TDNN, self).__init__()\n        \n        # Initial Conv1D + ReLU + BN\n        self.conv1 = nn.Conv1d(input_dim, channels, kernel_size=5, padding=2)\n        self.relu = nn.ReLU()\n        self.bn1 = nn.BatchNorm1d(channels)\n        \n        # Three SE-Res2Blocks with different dilations (2, 3, 4)\n        self.se_res2block1 = SERes2Block(\n            channels, channels, kernel_size=3, dilation=2, scale=res2net_scale\n        )\n        self.se_res2block2 = SERes2Block(\n            channels, channels, kernel_size=3, dilation=3, scale=res2net_scale\n        )\n        self.se_res2block3 = SERes2Block(\n            channels, channels, kernel_size=3, dilation=4, scale=res2net_scale\n        )\n        \n        # Multi-Layer Feature Aggregation + Conv1D\n        self.conv_agg = nn.Conv1d(3 * channels, 1536, kernel_size=1)\n        self.bn_agg = nn.BatchNorm1d(1536)\n        \n        # Attentive Statistics Pooling + BN\n        self.asp = AttentiveStatsPooling(1536)\n        self.bn_asp = nn.BatchNorm1d(3072)  # 1536*2 because we concatenate mean and std\n        \n        # Final FC layer + BN for embedding\n        self.fc = nn.Linear(3072, embedding_dim)\n        self.bn_emb = nn.BatchNorm1d(embedding_dim)\n\n    def forward(self, input_values=None, labels=None, attention_mask=None, **kwargs):\n        \"\"\"\n        Forward pass of the ECAPA-TDNN model\n        \n        Args:\n            x: Input tensor of shape (batch_size, time_steps, input_dim)\n               or (batch_size, input_dim, time_steps)\n        \n        Returns:\n            Speaker embedding of shape (batch_size, embedding_dim)\n        \"\"\"\n        # Check input shape and transpose if needed\n        x = input_values\n        if x.size(1) == self.conv1.in_channels:\n            # Input is already in the format (batch, channels, time)\n            pass\n        else:\n            # Input is in the format (batch, time, channels)\n            x = x.transpose(1, 2)\n        \n        # Initial Conv1D + ReLU + BN\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.bn1(x)\n        \n        # Three SE-Res2Blocks\n        x1 = self.se_res2block1(x)\n        x2 = self.se_res2block2(x1)\n        x3 = self.se_res2block3(x2)\n        \n        # Multi-Layer Feature Aggregation + Conv1D + ReLU + BN\n        x = torch.cat([x1, x2, x3], dim=1)\n        x = self.conv_agg(x)\n        x = self.relu(x)\n        x = self.bn_agg(x)\n        \n        # Attentive Statistics Pooling + BN\n        x = self.asp(x)\n        x = self.bn_asp(x)\n        \n        # Final FC layer + BN for embedding\n        x = self.fc(x)\n        x = self.bn_emb(x)\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T11:26:30.363416Z","iopub.execute_input":"2025-05-06T11:26:30.363676Z","iopub.status.idle":"2025-05-06T11:26:30.372558Z","shell.execute_reply.started":"2025-05-06T11:26:30.363655Z","shell.execute_reply":"2025-05-06T11:26:30.371798Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ResNet50","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport math\nimport torch.nn.functional as F\n\nclass ResNet50(nn.Module):\n    def __init__(self, embedding_dim=192, pretrained=False):\n        super(ResNet50, self).__init__()\n        # Load a pre-trained ResNet-50 without the final classification layer\n        self.resnet50 = models.resnet50(weights=\"IMAGENET1K_V2\" if pretrained else None)\n\n        # Modify the first convolutional layer for single-channel input\n        self.resnet50.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        \n        # Remove the fully connected layer\n        self.resnet50.fc = nn.Identity()\n        \n        # Add a new fully connected layer for speaker embeddings\n        self.embedding_layer = nn.Sequential(\n            nn.Linear(2048, embedding_dim),\n            nn.ReLU(),\n            nn.BatchNorm1d(embedding_dim)\n        )\n        \n    def forward(self, input_values=None, labels=None, attention_mask=None, **kwargs):\n        x = input_values\n        if x.size(1) == self.resnet50.conv1.in_channels:\n            # Input is already in the format (batch, channels, time)\n            pass\n        else:\n            # Input is in the format (batch, time, channels)\n            x = x.transpose(1, 2)\n        # Extract features using ResNet-50 backbone\n        features = self.resnet50(x)\n        # Generate speaker embeddings\n        embeddings = self.embedding_layer(features)\n        return embeddings\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:29:10.049003Z","iopub.execute_input":"2025-05-09T14:29:10.049869Z","iopub.status.idle":"2025-05-09T14:29:10.058906Z","shell.execute_reply.started":"2025-05-09T14:29:10.049834Z","shell.execute_reply":"2025-05-09T14:29:10.058256Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## AAM Softmax Loss","metadata":{}},{"cell_type":"code","source":"class AAMSoftmaxLoss(nn.Module):\n    def __init__(self, embedding_dim=192, num_speakers=1000, margin=0.2, scale=30):\n        \"\"\"\n        Additive Angular Margin Softmax Loss\n        \n        Args:\n            embedding_dim: Dimension of the embeddings\n            num_speakers: Number of speakers in the training set\n            margin: Angular margin penalty\n            scale: Scale factor for the cosine values\n        \"\"\"\n        super(AAMSoftmaxLoss, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.num_speakers = num_speakers\n        self.margin = margin\n        self.scale = scale\n        \n        # Weight for the speaker classification\n        self.weight = nn.Parameter(torch.FloatTensor(num_speakers, embedding_dim))\n        nn.init.xavier_normal_(self.weight, gain=1)\n        \n        # Pre-compute constants for efficiency\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n        \n    def forward(self, embeddings, labels):\n        # Input validation\n        # Ensure labels are proper long integers and within range\n        labels = labels.long()\n        if torch.any(labels < 0) or torch.any(labels >= self.num_speakers):\n            raise ValueError(f\"Labels must be in range [0, {self.num_speakers-1}], got: min={labels.min().item()}, max={labels.max().item()}\")\n        \n        # Normalize embeddings and weights\n        embeddings_norm = F.normalize(embeddings, p=2, dim=1)\n        weight_norm = F.normalize(self.weight, p=2, dim=1)\n        \n        # Compute cosine similarity\n        cosine = F.linear(embeddings_norm, weight_norm)\n        \n        # Add angular margin penalty\n        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        \n        # Apply one-hot encoding for the target labels\n        one_hot = torch.zeros_like(cosine)\n        one_hot.scatter_(1, labels.view(-1, 1), 1)\n        \n        # Apply margin to the target classes only\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        \n        # Scale the output\n        output = output * self.scale\n        \n        # Cross entropy loss - ensure labels are in the expected format\n        loss = F.cross_entropy(output, labels)\n        \n        return loss, output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:02:44.47187Z","iopub.execute_input":"2025-05-09T14:02:44.472143Z","iopub.status.idle":"2025-05-09T14:02:44.480325Z","shell.execute_reply.started":"2025-05-09T14:02:44.472117Z","shell.execute_reply":"2025-05-09T14:02:44.479497Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Datasets & Data Collator","metadata":{}},{"cell_type":"code","source":"pip install webrtcvad","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:02:48.071708Z","iopub.execute_input":"2025-05-09T14:02:48.071967Z","iopub.status.idle":"2025-05-09T14:02:55.488465Z","shell.execute_reply.started":"2025-05-09T14:02:48.07195Z","shell.execute_reply":"2025-05-09T14:02:55.48756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import webrtcvad\ndef remove_silence(waveform, sample_rate=16000, frame_duration_ms=30):\n    vad = webrtcvad.Vad(2)  # Moderate aggressiveness (0-3)\n    waveform_np = waveform.squeeze().numpy()\n    waveform_int16 = (waveform_np * 32767).astype(np.int16)\n    frame_length = int(sample_rate * frame_duration_ms / 1000)\n    frames = [waveform_int16[i:i+frame_length] for i in range(0, len(waveform_int16), frame_length)]\n    \n    voiced_frames = []\n    for frame in frames:\n        if len(frame) == frame_length and vad.is_speech(frame.tobytes(), sample_rate):\n            voiced_frames.append(frame)\n    \n    if voiced_frames:\n        voiced_waveform = np.concatenate(voiced_frames).astype(np.float32) / 32767\n        return torch.tensor(voiced_waveform, dtype=torch.float32).unsqueeze(0)\n    return waveform","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:03:10.126055Z","iopub.execute_input":"2025-05-09T14:03:10.126365Z","iopub.status.idle":"2025-05-09T14:03:10.215149Z","shell.execute_reply.started":"2025-05-09T14:03:10.12634Z","shell.execute_reply":"2025-05-09T14:03:10.214274Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom torch.utils.data import Dataset\nimport librosa\nimport torchaudio\n\n\nclass VietnamCelebDatasetTrain(Dataset):\n    def __init__(self, train_path, root_dir, sr=16000, duration=10):\n        self.root_dir = root_dir\n        self.filepaths = []\n        self.labels = []\n        self.meta = []\n        self.sr = sr\n        self.duration = duration\n        self.max_length = sr * duration\n        self.mfcc_transform = torchaudio.transforms.MFCC(sample_rate=16000,\n            n_mfcc=80,  # You may adjust this value depending on the number of MFCCs you want\n            melkwargs={\n                \"n_fft\": 512,\n                \"win_length\": 400,\n                \"hop_length\": 160,\n                \"window_fn\": torch.hamming_window,\n                \"n_mels\": 80,\n            }\n                                                        )\n        # Read file metadata\n        with open(train_path, 'r') as f:\n            for line in f:\n                speaker_id, audio_filename = line.strip().split()\n                audio_path = os.path.join(self.root_dir, speaker_id, audio_filename)\n                if os.path.exists(audio_path):\n                    self.filepaths.append(audio_path)\n                    self.labels.append(speaker_id)\n\n        self.label_map = {label: idx for idx, label in enumerate(sorted(set(self.labels)))}\n        self.labels = [self.label_map[label] for label in self.labels]\n\n\n    def __len__(self):\n        return len(self.filepaths)\n        \n    def __getitem__(self, idx):\n        # Load the waveform\n        wav_path = self.filepaths[idx]\n        waveform, sample_rate = torchaudio.load(wav_path)\n        waveform = remove_silence(waveform)\n        \n        if sample_rate != self.sr:\n            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.sr)\n            waveform = resampler(waveform)\n        waveform = waveform[0]  # Convert [1, N] to [N]\n\n        mfccs = self.mfcc_transform(waveform)\n\n        # Get label \n        label = self.labels[idx]\n\n        # Return as dictionary to match the format expected by the trainer\n        return {\n            'input_values': mfccs,\n            'speaker_labels': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:03:36.672042Z","iopub.execute_input":"2025-05-09T14:03:36.672734Z","iopub.status.idle":"2025-05-09T14:03:36.680945Z","shell.execute_reply.started":"2025-05-09T14:03:36.672712Z","shell.execute_reply":"2025-05-09T14:03:36.680079Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\nclass DataCollatorVietnamCeleb:\n    def __call__(self, batch):\n        input_values = [torch.tensor(item['input_values']) for item in batch]  # shape: [80, T]\n        labels = [item['speaker_labels'] for item in batch]  # adjust to your label format\n\n        # Pad MFCCs to the same time length (dim=2)\n        # Transpose to [T, 80] to pad along time dimension\n        input_values = [x.T for x in input_values]  # [T, 80]\n        input_padded = pad_sequence(input_values, batch_first=True)  # [B, T_max, 80]\n        input_padded = input_padded.transpose(1, 2)  # Back to [B, 80, T_max]\n        input_padded = input_padded.unsqueeze(1)\n\n        labels = torch.tensor(labels)  # or handle text/tokenized labels accordingly\n\n        return {\n            'input_values': input_padded,\n            'speaker_labels': labels\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:03:41.590741Z","iopub.execute_input":"2025-05-09T14:03:41.591246Z","iopub.status.idle":"2025-05-09T14:03:41.596288Z","shell.execute_reply.started":"2025-05-09T14:03:41.591218Z","shell.execute_reply":"2025-05-09T14:03:41.595723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import  DataLoader\nimport torch\ntrain_path = '/kaggle/input/vietnam-celeb-dataset/vietnam-celeb-t.txt'  # Folder lead to the Train Path\nroot_dir = '/kaggle/input/vietnam-celeb-dataset/full-dataset/data/'  # The folder to contain the audio file\ntrain_dataset = VietnamCelebDatasetTrain(train_path, root_dir)\n# Data Collator\ntrain_collator = DataCollatorVietnamCeleb()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:03:43.591711Z","iopub.execute_input":"2025-05-09T14:03:43.591998Z","iopub.status.idle":"2025-05-09T14:08:07.361734Z","shell.execute_reply.started":"2025-05-09T14:03:43.591978Z","shell.execute_reply":"2025-05-09T14:08:07.361113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_dataset[1]['input_values'].unsqueeze(1).shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T13:25:31.176963Z","iopub.execute_input":"2025-05-09T13:25:31.177719Z","iopub.status.idle":"2025-05-09T13:25:31.351861Z","shell.execute_reply.started":"2025-05-09T13:25:31.177688Z","shell.execute_reply":"2025-05-09T13:25:31.351159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VietnamCelebDatasetValidation(Dataset):\n    def __init__(self, val_path, root_dir, sr=16000, duration=10):\n        \"\"\"\n        Dataset for speaker verification validation using pre-defined utterance pairs\n        \n        Args:\n            val_path: Path to validation file with pre-defined pairs\n            root_dir: Root directory containing the audio files\n            sr: Sample rate\n            duration: Max duration in seconds\n        \"\"\"\n        self.root_dir = root_dir\n        self.sr = sr\n        self.duration = duration\n        self.max_length = sr * duration\n        self.mfcc_transform = torchaudio.transforms.MFCC(sample_rate=16000,\n            n_mfcc=80,  # You may adjust this value depending on the number of MFCCs you want\n            melkwargs={\n                \"n_fft\": 512,\n                \"win_length\": 400,\n                \"hop_length\": 160,\n                \"window_fn\": torch.hamming_window,\n                \"n_mels\": 80,\n            }\n                                                        )\n        # Store pairs and labels\n        self.pairs = []\n        self.labels = []\n        \n        # Read validation file with pairs\n        with open(val_path, 'r') as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) == 3:\n                    label, utt_path1, utt_path2 = parts\n                    \n                    # Create full paths\n                    audio_path1 = os.path.join(self.root_dir, utt_path1)\n                    audio_path2 = os.path.join(self.root_dir, utt_path2)\n                    \n                    # Check if both files exist\n                    if os.path.exists(audio_path1) and os.path.exists(audio_path2):\n                        self.pairs.append((audio_path1, audio_path2))\n                        self.labels.append(int(label))\n    \n    def __len__(self):\n        return len(self.pairs)\n    \n    def __getitem__(self, idx):\n        audio_path1, audio_path2 = self.pairs[idx]\n        label = self.labels[idx]\n        \n        # Load and process first waveform\n        mfccs1 = self._load_audio(audio_path1)\n        \n        # Load and process second waveform\n        mfccs2 = self._load_audio(audio_path2)\n        \n        return {\n            'input_values': mfccs1,\n            'input_values2': mfccs2,\n            'pair_labels': torch.tensor(label, dtype=torch.long)\n        }\n        \n    def _load_audio(self, path):\n        \"\"\"Load and preprocess audio file\"\"\"\n        waveform, sample_rate = torchaudio.load(path)\n        waveform = remove_silence(waveform)\n        \n        waveform = waveform[0]  # Convert [1, N] to [N]\n\n        mfccs = self.mfcc_transform(waveform)\n        \n        return mfccs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:09:41.781902Z","iopub.execute_input":"2025-05-09T14:09:41.782425Z","iopub.status.idle":"2025-05-09T14:09:41.790395Z","shell.execute_reply.started":"2025-05-09T14:09:41.7824Z","shell.execute_reply":"2025-05-09T14:09:41.789707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Collator for Validation\nclass ValidationDataCollator:\n    def __call__(self, batch):\n        input_values = [item['input_values'].T for item in batch]   # [T, 80]\n        input_values2 = [item['input_values2'].T for item in batch] # [T, 80]\n        pair_labels = torch.stack([item['pair_labels'] for item in batch])  # assume fixed-size\n\n        # Pad both input sets\n        input_values_padded = pad_sequence(input_values, batch_first=True)   # [B, T_max, 80]\n        input_values2_padded = pad_sequence(input_values2, batch_first=True) # [B, T_max, 80]\n\n        # Transpose back to [B, 80, T_max]\n        input_values_padded = input_values_padded.transpose(1, 2).unsqueeze(1)\n        input_values2_padded = input_values2_padded.transpose(1, 2).unsqueeze(1)\n\n        return {\n            'input_values': input_values_padded,\n            'input_values2': input_values2_padded,\n            'pair_labels': pair_labels\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:19:29.612205Z","iopub.execute_input":"2025-05-09T14:19:29.614063Z","iopub.status.idle":"2025-05-09T14:19:29.621731Z","shell.execute_reply.started":"2025-05-09T14:19:29.614027Z","shell.execute_reply":"2025-05-09T14:19:29.620922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nfrom torch.utils.data import Subset\n# Validation data\nval_path = \"/kaggle/input/vietnam-celeb-dataset/vietnam-celeb-e.txt\"\nval_dataset = VietnamCelebDatasetValidation(val_path, root_dir)\nsample_indices = random.sample(range(len(val_dataset)), 5000)\nval_dataset = Subset(val_dataset, indices=sample_indices)\nval_collator = ValidationDataCollator()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:08:56.454856Z","iopub.execute_input":"2025-05-09T14:08:56.455118Z","iopub.status.idle":"2025-05-09T14:09:13.740197Z","shell.execute_reply.started":"2025-05-09T14:08:56.455098Z","shell.execute_reply":"2025-05-09T14:09:13.739412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combined Data Collator to put both train and val Collator in one class\nclass CombinedDataCollator:\n    def __init__(self, train_collator, val_collator):\n        self.train_collator = train_collator\n        self.val_collator = val_collator\n        \n    def __call__(self, batch):\n        # Check if this is validation data by looking for input_values2\n        if isinstance(batch[0], dict) and 'input_values2' in batch[0]:\n            return self.val_collator(batch)\n        else:\n            return self.train_collator(batch)\ncombined_collator = CombinedDataCollator(train_collator, val_collator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:19:43.320892Z","iopub.execute_input":"2025-05-09T14:19:43.321402Z","iopub.status.idle":"2025-05-09T14:19:43.327958Z","shell.execute_reply.started":"2025-05-09T14:19:43.32137Z","shell.execute_reply":"2025-05-09T14:19:43.327094Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## WandB Setup","metadata":{}},{"cell_type":"code","source":"from safetensors.torch import load_file\n# Log model architecture to wandb\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = ECAPA_TDNN().to(device)\nstate_dict = load_file(\"/kaggle/input/ecapa_sv_hust/transformers/default/1/ecapa_05_05.safetensors\")\nmodel.load_state_dict(state_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T23:29:35.72554Z","iopub.execute_input":"2025-05-05T23:29:35.72633Z","iopub.status.idle":"2025-05-05T23:29:41.111166Z","shell.execute_reply.started":"2025-05-05T23:29:35.7263Z","shell.execute_reply":"2025-05-05T23:29:41.110226Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\nimport os\nimport torch\n\nos.environ[\"WANDB_KEY\"] = \"YOUR_WANDB_API_KEY\"\n\n\n# Initialize wandb\nwandb.login(key=os.getenv(\"WANDB_KEY\"))\nwandb.init(\n    project=\"RESNET_50_SV\",\n    name=\"resnet-50-training\",\n    config={\n        \"learning_rate\": 3e-4,\n        \"architecture\": \"RESNET50\",\n        \"dataset\": \"vietnam-celeb\",\n        \"epochs\": 5,\n    }\n)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = ResNet50().to(device)\n\nwandb.watch(model, log=\"all\")\n\n# Define batch size and other hyperparameters\nbatch_size = 8  \ntotal_epochs = 5\n\n# Update wandb config with additional hyperparameters\nwandb.config.update({\n    \"batch_size\": batch_size,\n    \"total_epochs\": total_epochs,\n    \"aam_margin\": 0.2,\n    \"aam_scale\": 30,\n    \"num_speakers\": 1000\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:50:40.949343Z","iopub.execute_input":"2025-05-09T14:50:40.949715Z","iopub.status.idle":"2025-05-09T14:50:41.405977Z","shell.execute_reply.started":"2025-05-09T14:50:40.949689Z","shell.execute_reply":"2025-05-09T14:50:41.405412Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate Method","metadata":{}},{"cell_type":"code","source":"def compute_eer(fnr, fpr, scores=None):\n    \"\"\"\n    Compute Equal Error Rate (EER) from false negative and false positive rates.\n    Returns: (eer, threshold)\n    \"\"\"\n    # Make sure fnr and fpr are numpy arrays\n    fnr = np.array(fnr)\n    fpr = np.array(fpr)\n    \n    # In case arrays are empty\n    if len(fnr) == 0 or len(fpr) == 0:\n        print(\"WARNING: Empty FNR or FPR arrays\")\n        return 0.5, 0.0  # Return default values\n    \n    # Calculate difference between FNR and FPR\n    diff = fnr - fpr\n    \n    # Find where the difference changes sign\n    # If diff changes sign, find the crossing point\n    if np.any(diff >= 0) and np.any(diff <= 0):\n        # Find indices where diff changes sign\n        positive_indices = np.flatnonzero(diff >= 0)\n        negative_indices = np.flatnonzero(diff <= 0)\n        \n        if len(positive_indices) > 0 and len(negative_indices) > 0:\n            # Get the boundary indices\n            idx1 = positive_indices[0]\n            idx2 = negative_indices[-1]\n            \n            # Check if indices are out of bounds\n            if idx1 >= len(fnr) or idx2 >= len(fnr):\n                print(\"WARNING: Index out of bounds\")\n                # Find closest points\n                abs_diff = np.abs(fnr - fpr)\n                min_idx = np.argmin(abs_diff)\n                eer = (fnr[min_idx] + fpr[min_idx]) / 2\n                threshold = scores[min_idx] if scores is not None else min_idx\n                return eer, threshold\n            \n            # Linear interpolation to find the EER\n            if fnr[idx1] == fpr[idx1]:\n                # Exactly equal at this point\n                eer = fnr[idx1]\n                threshold = scores[idx1] if scores is not None else idx1\n            else:\n                # Interpolate between idx1 and idx2\n                x = [fpr[idx2], fpr[idx1]]\n                y = [fnr[idx2], fnr[idx1]]\n                eer = np.mean(y)  # Approximate EER\n                \n                # If scores are provided, interpolate threshold\n                if scores is not None and idx1 < len(scores) and idx2 < len(scores):\n                    threshold = (scores[idx1] + scores[idx2]) / 2\n                else:\n                    threshold = (idx1 + idx2) / 2\n        else:\n            # Fallback if indices are not found\n            abs_diff = np.abs(fnr - fpr)\n            min_idx = np.argmin(abs_diff)\n            eer = (fnr[min_idx] + fpr[min_idx]) / 2\n            threshold = scores[min_idx] if scores is not None else min_idx\n    else:\n        # Fallback if no sign change - find the closest points\n        abs_diff = np.abs(fnr - fpr)\n        min_idx = np.argmin(abs_diff)\n        eer = (fnr[min_idx] + fpr[min_idx]) / 2\n        threshold = scores[min_idx] if scores is not None else min_idx\n    \n    return eer, threshold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:10:25.516059Z","iopub.execute_input":"2025-05-09T14:10:25.516347Z","iopub.status.idle":"2025-05-09T14:10:25.526289Z","shell.execute_reply.started":"2025-05-09T14:10:25.516324Z","shell.execute_reply":"2025-05-09T14:10:25.525571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_speaker_metrics(eval_pred):\n    \"\"\"Compute EER metrics for speaker verification.\"\"\"\n    # Extract embeddings and labels\n    embeddings1 = eval_pred.embeddings1\n    embeddings2 = eval_pred.embeddings2\n    pair_labels = eval_pred.labels\n    \n    # Compute similarity scores\n    similarity_scores = np.array([\n        np.dot(e1, e2) / (np.linalg.norm(e1) * np.linalg.norm(e2) + 1e-10)\n        for e1, e2 in zip(embeddings1, embeddings2)\n    ])\n    \n    # Compute FPR and FNR\n    thresholds = np.sort(similarity_scores)\n    fpr = np.zeros(len(thresholds))\n    fnr = np.zeros(len(thresholds))\n    \n    for i, threshold in enumerate(thresholds):\n        # Predictions based on threshold\n        pred = (similarity_scores >= threshold).astype(int)\n        \n        # True positives, false positives, true negatives, false negatives\n        tp = np.sum((pred == 1) & (pair_labels == 1))\n        fp = np.sum((pred == 1) & (pair_labels == 0))\n        tn = np.sum((pred == 0) & (pair_labels == 0))\n        fn = np.sum((pred == 0) & (pair_labels == 1))\n        \n        # FPR and FNR\n        fpr[i] = fp / (fp + tn) if (fp + tn) > 0 else 0\n        fnr[i] = fn / (fn + tp) if (fn + tp) > 0 else 0\n    \n    # Calculate EER\n    eer, threshold = compute_eer(fnr, fpr, similarity_scores)\n    result = {\n        \"eer\": eer,\n        \"eer_threshold\": threshold\n    }\n    # Log EER to wandb directly\n    wandb.log({\"eer\": eer})\n    \n    # Create and log DET curve to wandb\n    if len(fpr) > 10:  # Only log if we have enough points\n        \n        # Log histogram of similarity scores\n        try:\n            wandb.log({\n                \"similarity_scores\": wandb.Histogram(similarity_scores),\n                \"same_speaker_scores\": wandb.Histogram(similarity_scores[pair_labels == 1]),\n                \"diff_speaker_scores\": wandb.Histogram(similarity_scores[pair_labels == 0])\n            })\n        except Exception as e:\n            print(f\"Error logging histograms: {e}\")\n    \n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:10:28.415743Z","iopub.execute_input":"2025-05-09T14:10:28.416113Z","iopub.status.idle":"2025-05-09T14:10:28.43033Z","shell.execute_reply.started":"2025-05-09T14:10:28.41608Z","shell.execute_reply":"2025-05-09T14:10:28.429557Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Custom Trainer Object","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\nimport torch\nimport numpy as np\n\n# Custom Trainer implementation for speaker verification\nclass SpeakerVerificationTrainer(Trainer):\n    def __init__(self, *args, total_epochs=10, margin=0.2, scale=30, num_speakers=1000, **kwargs):\n        super().__init__(*args, **kwargs)\n        \n        self.total_epochs = total_epochs\n        self.margin = margin\n        self.scale = scale\n        self.num_speakers = num_speakers\n        embedding_dim = 192  # This gets the embedding dimension\n        print(embedding_dim)\n        \n        # Initialize AAMSoftmax criterion\n        self.criterion = AAMSoftmaxLoss(\n            embedding_dim=embedding_dim,  # Get embedding dim from model\n            num_speakers=num_speakers,\n            margin=margin,\n            scale=scale\n        ).to(self.args.device)\n        \n        # For storing embeddings during evaluation\n        self.pairs_embeddings1 = []\n        self.pairs_embeddings2 = []\n        self.pairs_labels = []\n        \n        # Log criterion parameters to wandb\n        wandb.config.update({\n            \"embedding_dim\": embedding_dim,\n            \"aam_margin\": margin,\n            \"aam_scale\": scale,\n            \"num_speakers\": num_speakers\n        })\n\n    def get_train_dataloader(self):\n        \"\"\"Create a working dataloader for training\"\"\"\n        # Create a simple dataloader that we know works\n        return DataLoader(\n            self.train_dataset, \n            batch_size=self.args.train_batch_size,\n            shuffle=True,\n            collate_fn=self.data_collator,\n            num_workers=4,  # Critical: use single process\n            pin_memory=False\n        )\n    \n    def get_eval_dataloader(self, eval_dataset=None):\n        \"\"\"Create a working dataloader for evaluation\"\"\"\n        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n        return DataLoader(\n            eval_dataset,\n            batch_size=self.args.eval_batch_size,\n            shuffle=False,\n            collate_fn=self.data_collator,\n            num_workers=4,  # Critical: use single process\n            pin_memory=False\n        )\n        \n    def scheduling(self, total_training_epoch, current_epoch, threshold=0.3):\n        \"\"\"Calculate the alpha value for the current epoch.\"\"\"\n        if total_training_epoch <= 1:\n            return threshold\n        alpha = (current_epoch - 1) / (total_training_epoch - 1)\n        return min(max(alpha, threshold), 1 - threshold)\n\n    def training_step(self, model, inputs, num_items=None):\n        \"\"\"Override training step to update alpha parameter.\"\"\"\n        # Get current epoch as integer (HF stores fractional)\n        current_epoch = int(self.state.epoch) + 1\n        new_alpha = self.scheduling(self.total_epochs, current_epoch)\n     \n        # Safely update alpha depending on model wrapping\n        if hasattr(model, 'module') and hasattr(model.module, 'alpha'):\n            model.module.alpha = new_alpha\n            alpha_value = model.module.alpha\n        elif hasattr(model, 'alpha'):\n            model.alpha = new_alpha\n            alpha_value = model.alpha\n        else:\n            alpha_value = None  # fallback\n    \n        # Print alpha update only at logging steps\n        if alpha_value is not None and self.state.global_step % self.args.logging_steps == 0:\n            self.log({\"alpha\": new_alpha})\n            print(f\"ðŸ” Epoch {current_epoch}: Alpha set to {alpha_value:.4f}\")\n            \n            # Also log to wandb\n            wandb.log({\"alpha\": new_alpha})\n    \n        # Let Trainer handle rest (loss computation, backprop, etc.)\n        return super().training_step(model, inputs, num_items)\n        \n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        \"\"\"Compute AAMSoftmax loss for the speaker embeddings.\"\"\"\n        # Extract inputs\n        input_values = inputs.get('input_values')\n        labels = inputs.get('speaker_labels')\n    \n        device = next(model.parameters()).device\n        input_values = input_values.to(device)\n        labels = labels.to(device) if labels is not None else None\n        \n        # Handle evaluation inputs with pairs for EER computation\n        is_eval_with_pairs = False\n        if not model.training and inputs.get('input_values2') is not None:\n            is_eval_with_pairs = True\n            input_values2 = inputs.get('input_values2').to(device)\n            pair_labels = inputs.get('pair_labels').to(device)\n        \n        # Forward pass to get speaker embeddings\n        embeddings = model(input_values)\n        \n        # Handle evaluation with pairs for EER\n        if is_eval_with_pairs:\n            # Get embeddings for second utterance in pairs\n            embeddings2 = model(input_values2)\n            \n            # Store pairs for EER calculation\n            self.pairs_embeddings1.append(embeddings.detach().cpu())\n            self.pairs_embeddings2.append(embeddings2.detach().cpu())\n            self.pairs_labels.append(pair_labels.detach().cpu())\n        \n        # Use AAMSoftmax loss for training\n        if labels is not None:\n            loss, outputs = self.criterion(embeddings, labels)\n            \n            # Log loss to wandb during training\n            if self.model.training and self.state.global_step % self.args.logging_steps == 0:\n                wandb.log({\"train/aam_loss\": loss.item()})\n        else:\n            loss = None\n            outputs = None\n        torch.cuda.empty_cache()\n        if return_outputs:\n            return loss, {\"loss\": loss, \"logits\": outputs, \"embeddings\": embeddings}\n        else:\n            return loss\n    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n        # Call the parent class method to get regular outputs\n        outputs = super().prediction_step(model, inputs, prediction_loss_only, ignore_keys)\n        \n        # During evaluation, collect embeddings for pairs\n        if not prediction_loss_only:\n            with torch.no_grad():\n                # Get embeddings from model (adjust based on your model's output structure)\n                device = next(model.parameters()).device\n                embeddings1 = model(inputs[\"input_values\"].to(device))\n                embeddings2 = model(inputs[\"input_values2\"].to(device))\n                \n                # Store embeddings and labels\n                self.pairs_embeddings1.append(embeddings1.detach().cpu())\n                self.pairs_embeddings2.append(embeddings2.detach().cpu())\n                self.pairs_labels.append(inputs[\"pair_labels\"].detach().cpu())\n        \n        return outputs    \n    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n        \"\"\"Override evaluate to compute EER at the end of evaluation.\"\"\"\n        # Reset storage for pairs\n        self.pairs_embeddings1 = []\n        self.pairs_embeddings2 = []\n        self.pairs_labels = []\n        \n        # Run standard evaluation\n        metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n        \n        # Calculate EER if we have collected pairs\n        if len(self.pairs_embeddings1) > 0:\n            # Prepare data for compute metrics function\n            embeddings1 = torch.cat(self.pairs_embeddings1, dim=0).numpy()\n            embeddings2 = torch.cat(self.pairs_embeddings2, dim=0).numpy()\n            pair_labels = torch.cat(self.pairs_labels, dim=0).numpy()\n            \n            # Create a container class to hold the embeddings\n            class EmbeddingPairs:\n                def __init__(self, embeddings1, embeddings2, labels):\n                    self.embeddings1 = embeddings1\n                    self.embeddings2 = embeddings2\n                    self.labels = labels\n            \n            eval_pairs = EmbeddingPairs(embeddings1, embeddings2, pair_labels)\n            \n            # Compute EER metrics\n            eer_metrics = compute_speaker_metrics(eval_pairs)\n            \n            # Add EER metrics to the overall metrics\n            for key, value in eer_metrics.items():\n                if key not in metrics:\n                    metrics[f\"{metric_key_prefix}_{key}\"] = value\n            \n            # Log to wandb with correct prefix\n            wandb_metrics = {\n                f\"{metric_key_prefix}/{key}\": value \n                for key, value in metrics.items() \n                if key.startswith(metric_key_prefix)\n            }\n            wandb.log(wandb_metrics)\n            \n            print(f\"\\n{metric_key_prefix.capitalize()} EER: {metrics.get(f'{metric_key_prefix}_eer', 0):.4f}\")\n            \n            # Log embedding visualizations to wandb (t-SNE of random subset)\n            if len(embeddings1) > 100:\n                try:\n                    from sklearn.manifold import TSNE\n                    # Sample a subset for visualization (for efficiency)\n                    max_samples = min(500, len(embeddings1))\n                    indices = np.random.choice(len(embeddings1), max_samples, replace=False)\n                    \n                    # Apply t-SNE\n                    tsne = TSNE(n_components=2, random_state=42)\n                    embeddings_combined = np.vstack([embeddings1[indices], embeddings2[indices]])\n                    embeddings_2d = tsne.fit_transform(embeddings_combined)\n                    \n                    # Split back into two sets\n                    n_samples = len(indices)\n                    embeddings1_2d = embeddings_2d[:n_samples]\n                    embeddings2_2d = embeddings_2d[n_samples:]\n                    \n                    # Create scatter plot data\n                    data = []\n                    for i in range(n_samples):\n                        data.append([\n                            embeddings1_2d[i, 0], embeddings1_2d[i, 1], \n                            \"Embedding 1\", int(pair_labels[indices[i]])\n                        ])\n                        data.append([\n                            embeddings2_2d[i, 0], embeddings2_2d[i, 1], \n                            \"Embedding 2\", int(pair_labels[indices[i]])\n                        ])\n                    \n                    # Log to wandb\n                    wandb.log({\n                        f\"{metric_key_prefix}/embeddings_tsne\": wandb.Table(\n                            data=data,\n                            columns=[\"x\", \"y\", \"embedding_type\", \"same_speaker\"]\n                        )\n                    })\n                except ImportError:\n                    print(\"sklearn not available for t-SNE visualization\")\n                except Exception as e:\n                    print(f\"Error creating t-SNE visualization: {e}\")\n        \n        return metrics\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:10:31.215719Z","iopub.execute_input":"2025-05-09T14:10:31.216301Z","iopub.status.idle":"2025-05-09T14:10:49.328386Z","shell.execute_reply.started":"2025-05-09T14:10:31.216279Z","shell.execute_reply":"2025-05-09T14:10:49.327824Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./checkpoint\",\n    per_device_train_batch_size=8,\n    eval_strategy=\"steps\",\n    save_strategy=\"epoch\",\n    logging_steps=400,\n    per_device_eval_batch_size=8,\n    learning_rate=3e-4,\n    gradient_accumulation_steps=4,\n    save_total_limit=5,\n    num_train_epochs=5,\n    dataloader_num_workers=4,\n    report_to=[\"wandb\"],  # Enable logging to wandb\n    metric_for_best_model=\"eval_eer\",\n    greater_is_better=False,  # Lower EER is better\n    fp16=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:51:48.365711Z","iopub.execute_input":"2025-05-09T14:51:48.365972Z","iopub.status.idle":"2025-05-09T14:51:48.402827Z","shell.execute_reply.started":"2025-05-09T14:51:48.365954Z","shell.execute_reply":"2025-05-09T14:51:48.401905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize trainer\ntrainer = SpeakerVerificationTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset, \n    eval_dataset=val_dataset,    \n    data_collator=combined_collator,  \n    compute_metrics=compute_speaker_metrics,\n    total_epochs=int(training_args.num_train_epochs),\n    num_speakers=1000,  # Adjust based on your dataset\n    margin=0.2,\n    scale=30\n)\n\n# Start training\ntrainer.train()\n\n# After training completes, save and log the best model to wandb\ntrainer.save_model(training_args.output_dir + \"/best_model\")\nwandb.save(training_args.output_dir + \"/best_model/*\")\n    \n\n# Finish the wandb run\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:51:51.715389Z","iopub.execute_input":"2025-05-09T14:51:51.71608Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nclass VoxVietnamDatasetTest(Dataset):\n    def __init__(self, test_path, root_dir, sr=16000, duration=10):\n        \"\"\"\n        Dataset for speaker verification validation using pre-defined utterance pairs\n        \n        Args:\n            val_path: Path to validation file with pre-defined pairs\n            root_dir: Root directory containing the audio files\n            sr: Sample rate\n            duration: Max duration in seconds\n        \"\"\"\n        self.root_dir = root_dir\n        self.sr = sr\n        self.duration = duration\n        self.max_length = sr * duration\n        self.mfcc_transform = torchaudio.transforms.MFCC(sample_rate=16000,\n            n_mfcc=80,  # You may adjust this value depending on the number of MFCCs you want\n            melkwargs={\n                \"n_fft\": 512,\n                \"win_length\": 400,\n                \"hop_length\": 160,\n                \"window_fn\": torch.hamming_window,\n                \"n_mels\": 80,\n            }\n                                                        )\n        \n        # Store pairs and labels\n        self.pairs = []\n        \n        # Read validation file with pairs\n        with open(test_path, 'r') as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) == 2:\n                    utt_path1, utt_path2 = parts\n                    \n                    # Create full paths\n                    audio_path1 = os.path.join(self.root_dir, utt_path1)\n                    audio_path2 = os.path.join(self.root_dir, utt_path2)\n                    \n                    # Check if both files exist\n                    if os.path.exists(audio_path1) and os.path.exists(audio_path2):\n                        self.pairs.append((audio_path1, audio_path2))\n    \n    def __len__(self):\n        return len(self.pairs)\n    \n    def __getitem__(self, idx):\n        audio_path1, audio_path2 = self.pairs[idx]\n        \n        # Load and process first waveform\n        mfccs1 = self._load_audio(audio_path1)\n        \n        # Load and process second waveform\n        mfccs2 = self._load_audio(audio_path2)\n        \n        return {\n            'input_values': mfccs1,\n            'input_values2': mfccs2\n        }\n        \n    def _load_audio(self, path):\n        \"\"\"Load and preprocess audio file\"\"\"\n        waveform, sample_rate = torchaudio.load(path)\n        waveform = remove_silence(waveform)\n        \n        waveform = waveform[0]  # Convert [1, N] to [N]\n\n        mfccs = self.mfcc_transform(waveform)\n        \n        return mfccs\nroot_test_dir = \"/kaggle/input/voxvietnam/wav/wav\"\ntest_path = \"/kaggle/input/voxvietnam/test_list.txt\"\ntest_dataset = VoxVietnamDatasetTest(test_path, root_test_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T11:32:01.397986Z","iopub.execute_input":"2025-05-06T11:32:01.398265Z","iopub.status.idle":"2025-05-06T11:33:06.680806Z","shell.execute_reply.started":"2025-05-06T11:32:01.398246Z","shell.execute_reply":"2025-05-06T11:33:06.680065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from safetensors.torch import load_file\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = ECAPA_TDNN().to(device)\n\nstate_dict = load_file(\"/kaggle/input/ecapa_sv_hust/transformers/default/3/ecapa_05_05_ckp_2835.safetensors\")\nmodel.load_state_dict(state_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T11:55:25.418853Z","iopub.execute_input":"2025-05-06T11:55:25.419448Z","iopub.status.idle":"2025-05-06T11:55:25.941972Z","shell.execute_reply.started":"2025-05-06T11:55:25.419425Z","shell.execute_reply":"2025-05-06T11:55:25.941266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.nn import CosineSimilarity\n\ncosine_sim = CosineSimilarity(dim = 1, eps = 1e-6)\n\nmodel.eval()\nwith torch.inference_mode():\n    for idx in range(len(test_dataset)):\n        data = test_dataset[idx]\n        test_data = data[\"input_values\"]\n        enroll_data = data[\"input_values2\"]\n\n        test_output = model(test_data.unsqueeze(0).to(device))\n        enroll_output = model(enroll_data.unsqueeze(0).to(device))\n\n        logit = cosine_sim(test_output, enroll_output)\n        with open(\"predictions.txt\", \"a\") as txtfile:\n            out_logit = logit[0].detach().item()\n            txtfile.write(f\"{out_logit:.2f}\")\n            txtfile.write(\"\\n\")\n        if (idx + 1) % 1000 == 0:\n            print(f\"Step : {idx + 1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T11:59:29.128626Z","iopub.execute_input":"2025-05-06T11:59:29.128918Z","iopub.status.idle":"2025-05-06T12:04:19.073293Z","shell.execute_reply.started":"2025-05-06T11:59:29.128873Z","shell.execute_reply":"2025-05-06T12:04:19.072527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"predictions.txt\", \"r\") as f:\n    print(len(f.read().splitlines()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T12:14:57.293021Z","iopub.execute_input":"2025-05-06T12:14:57.293683Z","iopub.status.idle":"2025-05-06T12:14:57.298716Z","shell.execute_reply.started":"2025-05-06T12:14:57.293659Z","shell.execute_reply":"2025-05-06T12:14:57.298178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}