{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9920101,"sourceType":"datasetVersion","datasetId":6096635}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport torch.nn as nn\nimport math\nimport torch\nimport torch.nn.functional as F\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:02:45.619583Z","iopub.execute_input":"2025-05-06T01:02:45.620135Z","iopub.status.idle":"2025-05-06T01:02:51.31592Z","shell.execute_reply.started":"2025-05-06T01:02:45.620105Z","shell.execute_reply":"2025-05-06T01:02:51.315192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install webrtcvad","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:02:51.317409Z","iopub.execute_input":"2025-05-06T01:02:51.31778Z","iopub.status.idle":"2025-05-06T01:02:58.78334Z","shell.execute_reply.started":"2025-05-06T01:02:51.317761Z","shell.execute_reply":"2025-05-06T01:02:58.782436Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocess and Dataset","metadata":{}},{"cell_type":"code","source":"train_path = '/kaggle/input/vietnam-celeb-dataset/vietnam-celeb-t.txt'  # Folder lead to the Train Path\nroot_dir = '/kaggle/input/vietnam-celeb-dataset/full-dataset/data/'  # The folder to contain the audio file\n\n\nimport os\nimport webrtcvad\nimport torchaudio\nimport numpy as np\ntransform = torchaudio.transforms.MFCC(sample_rate=16000,\n            n_mfcc=80,  # You may adjust this value depending on the number of MFCCs you want\n            melkwargs={\n                \"n_fft\": 512,\n                \"win_length\": 400,\n                \"hop_length\": 160,\n                \"f_min\": 20,\n                \"f_max\": 7600,\n                \"window_fn\": torch.hamming_window,\n                \"n_mels\": 80,\n            }\n        )\ndef mfcc_transform(waveform):\n    return transform(waveform)\n\ndef remove_silence(waveform, sample_rate=16000, frame_duration_ms=30):\n    vad = webrtcvad.Vad(2)  # Moderate aggressiveness (0-3)\n    waveform_np = waveform.squeeze().numpy()\n    waveform_int16 = (waveform_np * 32767).astype(np.int16)\n    frame_length = int(sample_rate * frame_duration_ms / 1000)\n    frames = [waveform_int16[i:i+frame_length] for i in range(0, len(waveform_int16), frame_length)]\n    \n    voiced_frames = []\n    for frame in frames:\n        if len(frame) == frame_length and vad.is_speech(frame.tobytes(), sample_rate):\n            voiced_frames.append(frame)\n    \n    if voiced_frames:\n        voiced_waveform = np.concatenate(voiced_frames).astype(np.float32) / 32767\n        return torch.tensor(voiced_waveform, dtype=torch.float32).unsqueeze(0)\n    return waveform","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:02:58.784584Z","iopub.execute_input":"2025-05-06T01:02:58.784869Z","iopub.status.idle":"2025-05-06T01:02:59.801259Z","shell.execute_reply.started":"2025-05-06T01:02:58.784845Z","shell.execute_reply":"2025-05-06T01:02:59.800661Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_audio(file_path, max_frames = 1000):\n    waveform, _ = torchaudio.load(file_path)\n    waveform = remove_silence(waveform, sample_rate=16000)\n    mfcc = mfcc_transform(waveform)  # [1, 80, num_frames]\n    \n    # Pad or truncate to max_frames\n    num_frames = mfcc.size(2)\n    if num_frames < max_frames:\n        padding = torch.zeros(1, 80, max_frames - num_frames)\n        mfcc = torch.cat([mfcc, padding], dim=2)  # [1, 80, max_frames]\n    elif num_frames > max_frames:\n        mfcc = mfcc[:, :, :max_frames]  # [1, 80, max_frames]\n    \n    return mfcc # [1, 80, max_frames]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:02:59.801993Z","iopub.execute_input":"2025-05-06T01:02:59.802228Z","iopub.status.idle":"2025-05-06T01:02:59.807798Z","shell.execute_reply.started":"2025-05-06T01:02:59.802211Z","shell.execute_reply":"2025-05-06T01:02:59.806938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom torch.utils.data import Dataset\n\n\nclass VietnamCelebDatasetTrain(Dataset):\n    def __init__(self, train_path, root_dir, sr=16000, duration=10):\n        self.root_dir = root_dir\n        self.filepaths = []\n        self.labels = []\n\n        with open(train_path, 'r') as f:\n            for line in f:\n                speaker_id, audio_filename = line.strip().split()\n                audio_path = os.path.join(self.root_dir, speaker_id, audio_filename)\n                if os.path.exists(audio_path):\n                    self.filepaths.append(audio_path)\n                    self.labels.append(speaker_id)\n\n        self.label_map = {label: idx for idx, label in enumerate(sorted(set(self.labels)))}\n        self.labels = [self.label_map[label] for label in self.labels]\n\n    def __len__(self):\n        return len(self.filepaths)\n        \n    def __getitem__(self, idx):\n        # Load the waveform\n        wav_path = self.filepaths[idx]\n        mfcc = preprocess_audio(wav_path)\n\n        # Get label and meta\n        label = self.labels[idx]\n\n        # Return as dictionary to match the format expected by the trainer\n        return {\n            'input_values': mfcc,\n            'speaker_labels': torch.tensor(label, dtype=torch.long),\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:02:59.809586Z","iopub.execute_input":"2025-05-06T01:02:59.809846Z","iopub.status.idle":"2025-05-06T01:02:59.828038Z","shell.execute_reply.started":"2025-05-06T01:02:59.809816Z","shell.execute_reply":"2025-05-06T01:02:59.827359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DataCollatorVietnamCeleb:\n    def __init__(self):\n        pass\n\n    def __call__(self, batch):\n        \"\"\"\n        Collates a batch of items into batched tensors.\n        \n        Args:\n            batch (List[Dict]): List of dictionaries from dataset\n            \n        Returns:\n            dict: Dictionary with batched tensors\n        \"\"\"\n        # Extract from batch\n        input_values = torch.stack([item['input_values'] for item in batch])\n        speaker_labels = torch.stack([item['speaker_labels'] for item in batch])\n        \n        # Create dictionary for model input\n        return {\n            'input_values': input_values,\n            'speaker_labels': speaker_labels,\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:02:59.828724Z","iopub.execute_input":"2025-05-06T01:02:59.828945Z","iopub.status.idle":"2025-05-06T01:02:59.842469Z","shell.execute_reply.started":"2025-05-06T01:02:59.82893Z","shell.execute_reply":"2025-05-06T01:02:59.841927Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntrain_collator = DataCollatorVietnamCeleb()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:02:59.84315Z","iopub.execute_input":"2025-05-06T01:02:59.843314Z","iopub.status.idle":"2025-05-06T01:02:59.922716Z","shell.execute_reply.started":"2025-05-06T01:02:59.843301Z","shell.execute_reply":"2025-05-06T01:02:59.922024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = VietnamCelebDatasetTrain(train_path, root_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:02:59.923368Z","iopub.execute_input":"2025-05-06T01:02:59.923635Z","iopub.status.idle":"2025-05-06T01:05:47.525507Z","shell.execute_reply.started":"2025-05-06T01:02:59.923612Z","shell.execute_reply":"2025-05-06T01:05:47.524053Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(train_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:05:47.525957Z","iopub.status.idle":"2025-05-06T01:05:47.526233Z","shell.execute_reply.started":"2025-05-06T01:05:47.526093Z","shell.execute_reply":"2025-05-06T01:05:47.526104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VietnamCelebDatasetValidation(Dataset):\n    def __init__(self, val_path, root_dir, sr=16000, duration=10):\n        \"\"\"\n        Dataset for speaker verification validation using pre-defined utterance pairs\n        \n        Args:\n            val_path: Path to validation file with pre-defined pairs\n            root_dir: Root directory containing the audio files\n            sr: Sample rate\n            duration: Max duration in seconds\n        \"\"\"\n        self.root_dir = root_dir\n        self.sr = sr\n        self.duration = duration\n        self.max_length = sr * duration\n        \n        # Store pairs and labels\n        self.pairs = []\n        self.labels = []\n        \n        # Read validation file with pairs\n        with open(val_path, 'r') as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) == 3:\n                    label, utt_path1, utt_path2 = parts\n                    \n                    # Create full paths\n                    audio_path1 = os.path.join(self.root_dir, utt_path1)\n                    audio_path2 = os.path.join(self.root_dir, utt_path2)\n                    \n                    # Check if both files exist\n                    if os.path.exists(audio_path1) and os.path.exists(audio_path2):\n                        self.pairs.append((audio_path1, audio_path2))\n                        self.labels.append(int(label))\n    \n    def __len__(self):\n        return len(self.pairs)\n    \n    def __getitem__(self, idx):\n        audio_path1, audio_path2 = self.pairs[idx]\n        label = self.labels[idx]\n        \n        # Load and process first waveform\n        mfcc1 = preprocess_audio(audio_path1)\n        \n        # Load and process second waveform\n        mfcc2 = preprocess_audio(audio_path2)\n        \n        return {\n            'input_values': mfcc1,\n            'input_values2': mfcc2,\n            'pair_labels': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:05:47.52771Z","iopub.status.idle":"2025-05-06T01:05:47.530536Z","shell.execute_reply.started":"2025-05-06T01:05:47.527841Z","shell.execute_reply":"2025-05-06T01:05:47.527856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ValidationDataCollator:\n    def __call__(self, batch):\n        input_values = torch.stack([item['input_values'] for item in batch])\n        input_values2 = torch.stack([item['input_values2'] for item in batch])\n        pair_labels = torch.stack([item['pair_labels'] for item in batch])\n        \n        return {\n            'input_values': input_values,\n            'input_values2': input_values2,\n            'pair_labels': pair_labels\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:05:47.531913Z","iopub.status.idle":"2025-05-06T01:05:47.53216Z","shell.execute_reply.started":"2025-05-06T01:05:47.532056Z","shell.execute_reply":"2025-05-06T01:05:47.532067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Subset\n# Validation data\nval_path = \"/kaggle/input/vietnam-celeb-dataset/vietnam-celeb-e.txt\"\nval_dataset = VietnamCelebDatasetValidation(val_path, root_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:05:47.533373Z","iopub.status.idle":"2025-05-06T01:05:47.533572Z","shell.execute_reply.started":"2025-05-06T01:05:47.533476Z","shell.execute_reply":"2025-05-06T01:05:47.533485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nsample_indices = random.sample(range(len(val_dataset)), 5000)\nval_dataset = Subset(val_dataset, indices=sample_indices)\nval_collator = ValidationDataCollator()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:05:47.534518Z","iopub.status.idle":"2025-05-06T01:05:47.534837Z","shell.execute_reply.started":"2025-05-06T01:05:47.534683Z","shell.execute_reply":"2025-05-06T01:05:47.534697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CombinedDataCollator:\n    def __init__(self, train_collator, val_collator):\n        self.train_collator = train_collator\n        self.val_collator = val_collator\n        \n    def __call__(self, batch):\n        # Check if this is validation data by looking for input_values2\n        if isinstance(batch[0], dict) and 'input_values2' in batch[0]:\n            return self.val_collator(batch)\n        else:\n            return self.train_collator(batch)\ncombined_collator = CombinedDataCollator(train_collator, val_collator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:05:47.536128Z","iopub.status.idle":"2025-05-06T01:05:47.536843Z","shell.execute_reply.started":"2025-05-06T01:05:47.536681Z","shell.execute_reply":"2025-05-06T01:05:47.536697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import  DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:05:47.537472Z","iopub.status.idle":"2025-05-06T01:05:47.537682Z","shell.execute_reply.started":"2025-05-06T01:05:47.537585Z","shell.execute_reply":"2025-05-06T01:05:47.537593Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Res2Net","metadata":{}},{"cell_type":"code","source":"class Bottle2neck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, baseWidth=26, scale = 4, stype='normal'):\n        \"\"\" Constructor\n        Args:\n            inplanes: input channel dimensionality\n            planes: output channel dimensionality\n            stride: conv stride. Replaces pooling layer.\n            downsample: None when stride = 1\n            baseWidth: basic width of conv3x3\n            scale: number of scale.\n            type: 'normal': normal set. 'stage': first block of a new stage.\n        \"\"\"\n        super(Bottle2neck, self).__init__()\n\n        width = int(math.floor(planes * (baseWidth/64.0)))\n        self.conv1 = nn.Conv2d(inplanes, width*scale, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width*scale)\n        \n        if scale == 1:\n          self.nums = 1\n        else:\n          self.nums = scale -1\n        if stype == 'stage':\n            self.pool = nn.AvgPool2d(kernel_size=3, stride = stride, padding=1)\n        convs = []\n        bns = []\n        for i in range(self.nums):\n          convs.append(nn.Conv2d(width, width, kernel_size=3, stride = stride, padding=1, bias=False))\n          bns.append(nn.BatchNorm2d(width))\n        self.convs = nn.ModuleList(convs)\n        self.bns = nn.ModuleList(bns)\n\n        self.conv3 = nn.Conv2d(width*scale, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stype = stype\n        self.scale = scale\n        self.width  = width\n\n    def forward(self, x):\n        \n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        spx = torch.split(out, self.width, 1)\n        for i in range(self.nums):\n          if i==0 or self.stype=='stage':\n            sp = spx[i]\n          else:\n            sp = sp + spx[i]\n          sp = self.convs[i](sp)\n          sp = self.relu(self.bns[i](sp))\n          if i==0:\n            out = sp\n          else:\n            out = torch.cat((out, sp), 1)\n        if self.scale != 1 and self.stype=='normal':\n          out = torch.cat((out, spx[self.nums]),1)\n        elif self.scale != 1 and self.stype=='stage':\n          out = torch.cat((out, self.pool(spx[self.nums])),1)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:05:47.538442Z","iopub.status.idle":"2025-05-06T01:05:47.538742Z","shell.execute_reply.started":"2025-05-06T01:05:47.538573Z","shell.execute_reply":"2025-05-06T01:05:47.538589Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Res2Net(nn.Module):\n    def __init__(self, block, layers, baseWidth=26, scale=4, embedding_size=256):\n        self.inplanes = 64\n        super(Res2Net, self).__init__()\n        self.baseWidth = baseWidth\n        self.scale = scale\n        # Single-channel input for MFCCs\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        # Embedding layer for speaker verification\n        self.embedding = nn.Linear(512 * block.expansion, embedding_size)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n                           stype='stage', baseWidth=self.baseWidth, scale=self.scale))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, baseWidth=self.baseWidth, scale=self.scale))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, input_values=None, labels=None, attention_mask=None, **kwargs):\n        x = input_values\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.embedding(x)  # Output embeddings\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:05:47.540213Z","iopub.status.idle":"2025-05-06T01:05:47.540451Z","shell.execute_reply.started":"2025-05-06T01:05:47.540333Z","shell.execute_reply":"2025-05-06T01:05:47.540343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchinfo import summary\nres2net50_26w_8s = Res2Net(Bottle2neck, [3, 4, 6, 3], baseWidth = 26, scale = 8)\nmodel = res2net50_26w_8s\nsummary(model = model, input_size  = [1, 1,224,224], col_names = [\"input_size\", \"output_size\", \"num_params\", \"trainable\"], row_settings = [\"var_names\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:05:47.541635Z","iopub.status.idle":"2025-05-06T01:05:47.541908Z","shell.execute_reply.started":"2025-05-06T01:05:47.541766Z","shell.execute_reply":"2025-05-06T01:05:47.541776Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loss Function","metadata":{}},{"cell_type":"code","source":"class AAMSoftmaxLoss(nn.Module):\n    def __init__(self, embedding_dim=256, num_speakers=1000, margin=0.2, scale=30):\n        super(AAMSoftmaxLoss, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.num_speakers = num_speakers\n        self.margin = margin\n        self.scale = scale\n        \n        # Weight for the speaker classification\n        self.weight = nn.Parameter(torch.FloatTensor(num_speakers, embedding_dim))\n        nn.init.xavier_normal_(self.weight, gain=1)\n        \n        # Pre-compute constants for efficiency\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n        \n    def forward(self, embeddings, labels):\n        # Normalize embeddings and weights\n        embeddings_norm = F.normalize(embeddings, p=2, dim=1)\n        weight_norm = F.normalize(self.weight, p=2, dim=1)\n        \n        # Compute cosine similarity\n        cosine = F.linear(embeddings_norm, weight_norm)\n        \n        # Add angular margin penalty\n        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        \n        # Apply one-hot encoding for the target labels\n        one_hot = torch.zeros_like(cosine)\n        one_hot.scatter_(1, labels.view(-1, 1), 1)\n        \n        # Apply margin to the target classes only\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        \n        # Scale the output\n        output = output * self.scale\n        \n        # Cross entropy loss\n        loss = F.cross_entropy(output, labels)\n        \n        return loss, output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:05:47.542721Z","iopub.status.idle":"2025-05-06T01:05:47.542928Z","shell.execute_reply.started":"2025-05-06T01:05:47.542826Z","shell.execute_reply":"2025-05-06T01:05:47.542834Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Metrics for evaluation","metadata":{}},{"cell_type":"code","source":"def compute_eer(fnr, fpr, scores=None):\n    \"\"\"\n    Compute Equal Error Rate (EER) from false negative and false positive rates.\n    Returns: (eer, threshold)\n    \"\"\"\n    # Make sure fnr and fpr are numpy arrays\n    fnr = np.array(fnr)\n    fpr = np.array(fpr)\n    \n    # In case arrays are empty\n    if len(fnr) == 0 or len(fpr) == 0:\n        print(\"WARNING: Empty FNR or FPR arrays\")\n        return 0.5, 0.0  # Return default values\n    \n    # Calculate difference between FNR and FPR\n    diff = fnr - fpr\n    \n    # Find where the difference changes sign\n    # If diff changes sign, find the crossing point\n    if np.any(diff >= 0) and np.any(diff <= 0):\n        # Find indices where diff changes sign\n        positive_indices = np.flatnonzero(diff >= 0)\n        negative_indices = np.flatnonzero(diff <= 0)\n        \n        if len(positive_indices) > 0 and len(negative_indices) > 0:\n            # Get the boundary indices\n            idx1 = positive_indices[0]\n            idx2 = negative_indices[-1]\n            \n            # Check if indices are out of bounds\n            if idx1 >= len(fnr) or idx2 >= len(fnr):\n                print(\"WARNING: Index out of bounds\")\n                # Find closest points\n                abs_diff = np.abs(fnr - fpr)\n                min_idx = np.argmin(abs_diff)\n                eer = (fnr[min_idx] + fpr[min_idx]) / 2\n                threshold = scores[min_idx] if scores is not None else min_idx\n                return eer, threshold\n            \n            # Linear interpolation to find the EER\n            if fnr[idx1] == fpr[idx1]:\n                # Exactly equal at this point\n                eer = fnr[idx1]\n                threshold = scores[idx1] if scores is not None else idx1\n            else:\n                # Interpolate between idx1 and idx2\n                x = [fpr[idx2], fpr[idx1]]\n                y = [fnr[idx2], fnr[idx1]]\n                eer = np.mean(y)  # Approximate EER\n                \n                # If scores are provided, interpolate threshold\n                if scores is not None and idx1 < len(scores) and idx2 < len(scores):\n                    threshold = (scores[idx1] + scores[idx2]) / 2\n                else:\n                    threshold = (idx1 + idx2) / 2\n        else:\n            # Fallback if indices are not found\n            abs_diff = np.abs(fnr - fpr)\n            min_idx = np.argmin(abs_diff)\n            eer = (fnr[min_idx] + fpr[min_idx]) / 2\n            threshold = scores[min_idx] if scores is not None else min_idx\n    else:\n        # Fallback if no sign change - find the closest points\n        abs_diff = np.abs(fnr - fpr)\n        min_idx = np.argmin(abs_diff)\n        eer = (fnr[min_idx] + fpr[min_idx]) / 2\n        threshold = scores[min_idx] if scores is not None else min_idx\n    \n    return eer, threshold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:05:47.543903Z","iopub.status.idle":"2025-05-06T01:05:47.544182Z","shell.execute_reply.started":"2025-05-06T01:05:47.544072Z","shell.execute_reply":"2025-05-06T01:05:47.544086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_speaker_metrics(eval_pred):\n    \"\"\"Compute EER metrics for speaker verification.\"\"\"\n    # Extract embeddings and labels\n    embeddings1 = eval_pred.embeddings1\n    embeddings2 = eval_pred.embeddings2\n    pair_labels = eval_pred.labels\n    \n    # Compute similarity scores\n    similarity_scores = np.array([\n        np.dot(e1, e2) / (np.linalg.norm(e1) * np.linalg.norm(e2) + 1e-10)\n        for e1, e2 in zip(embeddings1, embeddings2)\n    ])\n    \n    # Compute FPR and FNR\n    thresholds = np.sort(similarity_scores)\n    fpr = np.zeros(len(thresholds))\n    fnr = np.zeros(len(thresholds))\n    \n    for i, threshold in enumerate(thresholds):\n        # Predictions based on threshold\n        pred = (similarity_scores >= threshold).astype(int)\n        \n        # True positives, false positives, true negatives, false negatives\n        tp = np.sum((pred == 1) & (pair_labels == 1))\n        fp = np.sum((pred == 1) & (pair_labels == 0))\n        tn = np.sum((pred == 0) & (pair_labels == 0))\n        fn = np.sum((pred == 0) & (pair_labels == 1))\n        \n        # FPR and FNR\n        fpr[i] = fp / (fp + tn) if (fp + tn) > 0 else 0\n        fnr[i] = fn / (fn + tp) if (fn + tp) > 0 else 0\n    \n    # Calculate EER\n    eer, threshold = compute_eer(fnr, fpr, similarity_scores)\n    result = {\n        \"eer\": eer,\n        \"eer_threshold\": threshold\n    }\n    # Log EER to wandb directly\n    wandb.log({\"eer\": eer})\n    \n    # Create and log DET curve to wandb\n    if len(fpr) > 10:  # Only log if we have enough points\n        \n        # Log histogram of similarity scores\n        try:\n            wandb.log({\n                \"similarity_scores\": wandb.Histogram(similarity_scores),\n                \"same_speaker_scores\": wandb.Histogram(similarity_scores[pair_labels == 1]),\n                \"diff_speaker_scores\": wandb.Histogram(similarity_scores[pair_labels == 0])\n            })\n        except Exception as e:\n            print(f\"Error logging histograms: {e}\")\n    \n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:05:47.544975Z","iopub.status.idle":"2025-05-06T01:05:47.54528Z","shell.execute_reply.started":"2025-05-06T01:05:47.545128Z","shell.execute_reply":"2025-05-06T01:05:47.545143Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Wandb","metadata":{}},{"cell_type":"code","source":"import wandb\nimport os\nos.environ[\"WANDB_API_KEY\"] = \"YOUR_WANDB_API_KEY\"\nwandb.init(\n    project = \"Res2Net\",\n    name=\"Res2Net training 1\",\n    config={\n        \"learning_rate\": 3e-4,\n        \"architecture\": \"Res2Net\",\n        \"dataset\": \"vietnam-celeb\",\n        \"epochs\": 5,\n    }\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:05:47.546534Z","iopub.status.idle":"2025-05-06T01:05:47.546813Z","shell.execute_reply.started":"2025-05-06T01:05:47.546653Z","shell.execute_reply":"2025-05-06T01:05:47.546667Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Trainer","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\nimport torch\nimport numpy as np\n\n# Custom Trainer implementation for speaker verification\nclass SpeakerVerificationTrainer(Trainer):\n    def __init__(self, *args, total_epochs=10, margin=0.2, scale=30, num_speakers=1000, **kwargs):\n        super().__init__(*args, **kwargs)\n        \n        self.total_epochs = total_epochs\n        self.margin = margin\n        self.scale = scale\n        self.num_speakers = num_speakers\n        embedding_dim = 256  # This gets the embedding dimension\n        print(embedding_dim)\n        \n        # Initialize AAMSoftmax criterion\n        self.criterion = AAMSoftmaxLoss(\n            embedding_dim=embedding_dim,  # Get embedding dim from model\n            num_speakers=num_speakers,\n            margin=margin,\n            scale=scale\n        ).to(self.args.device)\n        \n        # For storing embeddings during evaluation\n        self.pairs_embeddings1 = []\n        self.pairs_embeddings2 = []\n        self.pairs_labels = []\n        \n        # Log criterion parameters to wandb\n        wandb.config.update({\n            \"embedding_dim\": embedding_dim,\n            \"aam_margin\": margin,\n            \"aam_scale\": scale,\n            \"num_speakers\": num_speakers\n        })\n\n    def get_train_dataloader(self):\n        \"\"\"Create a working dataloader for training\"\"\"\n        # Create a simple dataloader that we know works\n        return DataLoader(\n            self.train_dataset, \n            batch_size=self.args.train_batch_size,\n            shuffle=True,\n            collate_fn=self.data_collator,\n            num_workers=4,  # Critical: use single process\n            pin_memory=False\n        )\n    \n    def get_eval_dataloader(self, eval_dataset=None):\n        \"\"\"Create a working dataloader for evaluation\"\"\"\n        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n        return DataLoader(\n            eval_dataset,\n            batch_size=self.args.eval_batch_size,\n            shuffle=False,\n            collate_fn=self.data_collator,\n            num_workers=4,  # Critical: use single process\n            pin_memory=False\n        )\n        \n    def scheduling(self, total_training_epoch, current_epoch, threshold=0.3):\n        \"\"\"Calculate the alpha value for the current epoch.\"\"\"\n        if total_training_epoch <= 1:\n            return threshold\n        alpha = (current_epoch - 1) / (total_training_epoch - 1)\n        return min(max(alpha, threshold), 1 - threshold)\n\n    def training_step(self, model, inputs, num_items=None):\n        \"\"\"Override training step to update alpha parameter.\"\"\"\n        # Get current epoch as integer (HF stores fractional)\n        current_epoch = int(self.state.epoch) + 1\n        new_alpha = self.scheduling(self.total_epochs, current_epoch)\n     \n        # Safely update alpha depending on model wrapping\n        if hasattr(model, 'module') and hasattr(model.module, 'alpha'):\n            model.module.alpha = new_alpha\n            alpha_value = model.module.alpha\n        elif hasattr(model, 'alpha'):\n            model.alpha = new_alpha\n            alpha_value = model.alpha\n        else:\n            alpha_value = None  # fallback\n    \n        # Print alpha update only at logging steps\n        if alpha_value is not None and self.state.global_step % self.args.logging_steps == 0:\n            self.log({\"alpha\": new_alpha})\n            print(f\"ðŸ” Epoch {current_epoch}: Alpha set to {alpha_value:.4f}\")\n            \n            # Also log to wandb\n            wandb.log({\"alpha\": new_alpha})\n    \n        # Let Trainer handle rest (loss computation, backprop, etc.)\n        return super().training_step(model, inputs, num_items)\n        \n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        \"\"\"Compute AAMSoftmax loss for the speaker embeddings.\"\"\"\n        # Extract inputs\n        input_values = inputs.get('input_values')\n        labels = inputs.get('speaker_labels')\n    \n        device = next(model.parameters()).device\n        input_values = input_values.to(device)\n        labels = labels.to(device) if labels is not None else None\n        \n        # Handle evaluation inputs with pairs for EER computation\n        is_eval_with_pairs = False\n        if not model.training and inputs.get('input_values2') is not None:\n            is_eval_with_pairs = True\n            input_values2 = inputs.get('input_values2').to(device)\n            pair_labels = inputs.get('pair_labels').to(device)\n        \n        # Forward pass to get speaker embeddings\n        embeddings = model(input_values)\n        \n        # Handle evaluation with pairs for EER\n        if is_eval_with_pairs:\n            # Get embeddings for second utterance in pairs\n            embeddings2 = model(input_values2)\n            \n            # Store pairs for EER calculation\n            self.pairs_embeddings1.append(embeddings.detach().cpu())\n            self.pairs_embeddings2.append(embeddings2.detach().cpu())\n            self.pairs_labels.append(pair_labels.detach().cpu())\n        \n        # Use AAMSoftmax loss for training\n        if labels is not None:\n            loss, outputs = self.criterion(embeddings, labels)\n            \n            # Log loss to wandb during training\n            if self.model.training and self.state.global_step % self.args.logging_steps == 0:\n                wandb.log({\"train/aam_loss\": loss.item()})\n        else:\n            loss = None\n            outputs = None\n        torch.cuda.empty_cache()\n        if return_outputs:\n            return loss, {\"loss\": loss, \"logits\": outputs, \"embeddings\": embeddings}\n        else:\n            return loss\n    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n        # Call the parent class method to get regular outputs\n        outputs = super().prediction_step(model, inputs, prediction_loss_only, ignore_keys)\n        \n        # During evaluation, collect embeddings for pairs\n        if not prediction_loss_only:\n            with torch.no_grad():\n                # Get embeddings from model (adjust based on your model's output structure)\n                device = next(model.parameters()).device\n                embeddings1 = model(inputs[\"input_values\"].to(device))\n                embeddings2 = model(inputs[\"input_values2\"].to(device))\n                \n                # Store embeddings and labels\n                self.pairs_embeddings1.append(embeddings1.detach().cpu())\n                self.pairs_embeddings2.append(embeddings2.detach().cpu())\n                self.pairs_labels.append(inputs[\"pair_labels\"].detach().cpu())\n        \n        return outputs    \n    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n        \"\"\"Override evaluate to compute EER at the end of evaluation.\"\"\"\n        # Reset storage for pairs\n        self.pairs_embeddings1 = []\n        self.pairs_embeddings2 = []\n        self.pairs_labels = []\n        \n        # Run standard evaluation\n        metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n        \n        # Calculate EER if we have collected pairs\n        if len(self.pairs_embeddings1) > 0:\n            # Prepare data for compute metrics function\n            embeddings1 = torch.cat(self.pairs_embeddings1, dim=0).numpy()\n            embeddings2 = torch.cat(self.pairs_embeddings2, dim=0).numpy()\n            pair_labels = torch.cat(self.pairs_labels, dim=0).numpy()\n            \n            # Create a container class to hold the embeddings\n            class EmbeddingPairs:\n                def __init__(self, embeddings1, embeddings2, labels):\n                    self.embeddings1 = embeddings1\n                    self.embeddings2 = embeddings2\n                    self.labels = labels\n            \n            eval_pairs = EmbeddingPairs(embeddings1, embeddings2, pair_labels)\n            \n            # Compute EER metrics\n            eer_metrics = compute_speaker_metrics(eval_pairs)\n            \n            # Add EER metrics to the overall metrics\n            for key, value in eer_metrics.items():\n                if key not in metrics:\n                    metrics[f\"{metric_key_prefix}_{key}\"] = value\n            \n            # Log to wandb with correct prefix\n            wandb_metrics = {\n                f\"{metric_key_prefix}/{key}\": value \n                for key, value in metrics.items() \n                if key.startswith(metric_key_prefix)\n            }\n            wandb.log(wandb_metrics)\n            \n            print(f\"\\n{metric_key_prefix.capitalize()} EER: {metrics.get(f'{metric_key_prefix}_eer', 0):.4f}\")\n            \n            # Log embedding visualizations to wandb (t-SNE of random subset)\n            if len(embeddings1) > 100:\n                try:\n                    from sklearn.manifold import TSNE\n                    # Sample a subset for visualization (for efficiency)\n                    max_samples = min(500, len(embeddings1))\n                    indices = np.random.choice(len(embeddings1), max_samples, replace=False)\n                    \n                    # Apply t-SNE\n                    tsne = TSNE(n_components=2, random_state=42)\n                    embeddings_combined = np.vstack([embeddings1[indices], embeddings2[indices]])\n                    embeddings_2d = tsne.fit_transform(embeddings_combined)\n                    \n                    # Split back into two sets\n                    n_samples = len(indices)\n                    embeddings1_2d = embeddings_2d[:n_samples]\n                    embeddings2_2d = embeddings_2d[n_samples:]\n                    \n                    # Create scatter plot data\n                    data = []\n                    for i in range(n_samples):\n                        data.append([\n                            embeddings1_2d[i, 0], embeddings1_2d[i, 1], \n                            \"Embedding 1\", int(pair_labels[indices[i]])\n                        ])\n                        data.append([\n                            embeddings2_2d[i, 0], embeddings2_2d[i, 1], \n                            \"Embedding 2\", int(pair_labels[indices[i]])\n                        ])\n                    \n                    # Log to wandb\n                    wandb.log({\n                        f\"{metric_key_prefix}/embeddings_tsne\": wandb.Table(\n                            data=data,\n                            columns=[\"x\", \"y\", \"embedding_type\", \"same_speaker\"]\n                        )\n                    })\n                except ImportError:\n                    print(\"sklearn not available for t-SNE visualization\")\n                except Exception as e:\n                    print(f\"Error creating t-SNE visualization: {e}\")\n        \n        return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:05:47.548294Z","iopub.status.idle":"2025-05-06T01:05:47.548571Z","shell.execute_reply.started":"2025-05-06T01:05:47.548464Z","shell.execute_reply":"2025-05-06T01:05:47.548475Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./hubert_ecapa_tdnn\",\n    per_device_train_batch_size=8,\n    eval_strategy=\"steps\",\n    save_strategy=\"epoch\",\n    logging_steps=400,\n    per_device_eval_batch_size=8,\n    dataloader_num_workers= 4,\n    learning_rate=3e-4,\n    gradient_accumulation_steps=4,\n    save_total_limit=2,\n    num_train_epochs=5,\n    report_to=[\"wandb\"],  # Enable logging to wandb\n    metric_for_best_model=\"eval_eer\",\n    greater_is_better=False,  # Lower EER is better\n    fp16=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:05:47.549918Z","iopub.status.idle":"2025-05-06T01:05:47.550212Z","shell.execute_reply.started":"2025-05-06T01:05:47.550054Z","shell.execute_reply":"2025-05-06T01:05:47.55007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize trainer\ntrainer = SpeakerVerificationTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset, \n    eval_dataset=val_dataset,    \n    data_collator=combined_collator,  \n    compute_metrics=compute_speaker_metrics,\n    total_epochs=int(training_args.num_train_epochs),\n    num_speakers=1000,  # Adjust based on your dataset\n    margin=0.2,\n    scale=30\n)\n\n# Start training\ntrainer.train()\n\n# After training completes, save and log the best model to wandb\ntrainer.save_model(training_args.output_dir + \"/best_model\")\nwandb.save(training_args.output_dir + \"/best_model/*\")\n\n# Finish the wandb run\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:05:47.55129Z","iopub.status.idle":"2025-05-06T01:05:47.551576Z","shell.execute_reply.started":"2025-05-06T01:05:47.551446Z","shell.execute_reply":"2025-05-06T01:05:47.551461Z"}},"outputs":[],"execution_count":null}]}